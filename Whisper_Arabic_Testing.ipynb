{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyO7xefwWKGg7muaZzeYgKF/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/welsakka/ArabicEnglishLectureModel/blob/main/Whisper_Arabic_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "dgsxuhL0907P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-N3muOQo4QfM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141567b4-b01a-4873-bc60-a8dd2fc8fe68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20230314.tar.gz (792 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.9/792.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.56.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken==0.3.1 (from openai-whisper)\n",
            "  Downloading tiktoken-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python==0.2.0 (from openai-whisper)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python==0.2.0->openai-whisper) (0.18.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.1->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.3.1->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (3.12.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->openai-whisper) (16.0.6)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=796907 sha256=c45e2fd41175987729ab82e2258a1c977ee93ff63e2713c060e7e3d3c6447b1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/13/5f/fe8245f6dc59df505879da4b2129932e342f02a80e6b87f27d\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: ffmpeg-python, tiktoken, openai-whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 openai-whisper-20230314 tiktoken-0.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load custom model to use for Whisper"
      ],
      "metadata": {
        "id": "Mqdh_rlP96aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import re\n",
        "import torch\n",
        "from google.colab import drive\n",
        "# Mount google drive to output checkpoint file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def hf_to_whisper_states(text):\n",
        "    text = re.sub('.layers.', '.blocks.', text)\n",
        "    text = re.sub('.self_attn.', '.attn.', text)\n",
        "    text = re.sub('.q_proj.', '.query.', text)\n",
        "    text = re.sub('.k_proj.', '.key.', text)\n",
        "    text = re.sub('.v_proj.', '.value.', text)\n",
        "    text = re.sub('.out_proj.', '.out.', text)\n",
        "    text = re.sub('.fc1.', '.mlp.0.', text)\n",
        "    text = re.sub('.fc2.', '.mlp.2.', text)\n",
        "    text = re.sub('.fc3.', '.mlp.3.', text)\n",
        "    text = re.sub('.fc3.', '.mlp.3.', text)\n",
        "    text = re.sub('.encoder_attn.', '.cross_attn.', text)\n",
        "    text = re.sub('.cross_attn.ln.', '.cross_attn_ln.', text)\n",
        "    text = re.sub('.embed_positions.weight', '.positional_embedding', text)\n",
        "    text = re.sub('.embed_tokens.', '.token_embedding.', text)\n",
        "    text = re.sub('model.', '', text)\n",
        "    text = re.sub('attn.layer_norm.', 'attn_ln.', text)\n",
        "    text = re.sub('.final_layer_norm.', '.mlp_ln.', text)\n",
        "    text = re.sub('encoder.layer_norm.', 'encoder.ln_post.', text)\n",
        "    text = re.sub('decoder.layer_norm.', 'decoder.ln.', text)\n",
        "    return text\n",
        "\n",
        "# Load HF Model\n",
        "hf_state_dict = torch.load(\"/content/drive/MyDrive/AI_output/checkpoint-4000/pytorch_model.bin\")    # pytorch_model.bin file\n",
        "\n",
        "# Rename layers\n",
        "for key in list(hf_state_dict.keys())[:]:\n",
        "    new_key = hf_to_whisper_states(key)\n",
        "    hf_state_dict[new_key] = hf_state_dict.pop(key)\n",
        "\n",
        "#pop unneeded keys\n",
        "hf_state_dict.pop('proj_out.weight')\n",
        "\n",
        "# Init Whisper Model and replace model weights\n",
        "whisper_model = whisper.load_model('base')\n",
        "whisper_model.load_state_dict(hf_state_dict)"
      ],
      "metadata": {
        "id": "KY0OmAuc9_HW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428dc16b-f8f9-4ea4-cacc-a8024ecb21fd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 134MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " bi Allahi fughi waas'i yu kan do do that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model with audio file and print results"
      ],
      "metadata": {
        "id": "Tp3UgCTdRhm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = whisper_model.transcribe(\"/content/drive/MyDrive/AI Training/Arabic transliteration/mmtest.mp3\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "-kh6lWiFRgvK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}