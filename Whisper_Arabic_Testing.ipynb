{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNZOxLf1/iOZtENq/vspw9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/welsakka/ArabicEnglishLectureModel/blob/main/Whisper_Arabic_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install"
      ],
      "metadata": {
        "id": "dgsxuhL0907P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N3muOQo4QfM"
      },
      "outputs": [],
      "source": [
        "!pip install openai-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load custom model to use for Whisper"
      ],
      "metadata": {
        "id": "Mqdh_rlP96aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import re\n",
        "import torch\n",
        "from google.colab import drive\n",
        "# Mount google drive to output checkpoint file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def hf_to_whisper_states(text):\n",
        "    text = re.sub('.layers.', '.blocks.', text)\n",
        "    text = re.sub('.self_attn.', '.attn.', text)\n",
        "    text = re.sub('.q_proj.', '.query.', text)\n",
        "    text = re.sub('.k_proj.', '.key.', text)\n",
        "    text = re.sub('.v_proj.', '.value.', text)\n",
        "    text = re.sub('.out_proj.', '.out.', text)\n",
        "    text = re.sub('.fc1.', '.mlp.0.', text)\n",
        "    text = re.sub('.fc2.', '.mlp.2.', text)\n",
        "    text = re.sub('.fc3.', '.mlp.3.', text)\n",
        "    text = re.sub('.fc3.', '.mlp.3.', text)\n",
        "    text = re.sub('.encoder_attn.', '.cross_attn.', text)\n",
        "    text = re.sub('.cross_attn.ln.', '.cross_attn_ln.', text)\n",
        "    text = re.sub('.embed_positions.weight', '.positional_embedding', text)\n",
        "    text = re.sub('.embed_tokens.', '.token_embedding.', text)\n",
        "    text = re.sub('model.', '', text)\n",
        "    text = re.sub('attn.layer_norm.', 'attn_ln.', text)\n",
        "    text = re.sub('.final_layer_norm.', '.mlp_ln.', text)\n",
        "    text = re.sub('encoder.layer_norm.', 'encoder.ln_post.', text)\n",
        "    text = re.sub('decoder.layer_norm.', 'decoder.ln.', text)\n",
        "    return text\n",
        "\n",
        "# Load HF Model\n",
        "# Load the pytorch_model.bin file found in fine-tuned model files\n",
        "# e.g ..../checkpoint-8000/pytorch_model.bin\n",
        "hf_state_dict = torch.load(\"/path/to/pytorch_model.bin\")    # pytorch_model.bin file\n",
        "\n",
        "# Rename layers\n",
        "for key in list(hf_state_dict.keys())[:]:\n",
        "    new_key = hf_to_whisper_states(key)\n",
        "    hf_state_dict[new_key] = hf_state_dict.pop(key)\n",
        "\n",
        "#pop unneeded keys\n",
        "hf_state_dict.pop('proj_out.weight')\n",
        "\n",
        "# Init Whisper Model and replace model weights\n",
        "whisper_model = whisper.load_model('base')\n",
        "whisper_model.load_state_dict(hf_state_dict)"
      ],
      "metadata": {
        "id": "KY0OmAuc9_HW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Model with audio file and print results"
      ],
      "metadata": {
        "id": "Tp3UgCTdRhm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = whisper_model.transcribe(\"link/to/audiofile.mp3\")\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "id": "-kh6lWiFRgvK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}