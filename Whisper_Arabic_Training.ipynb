{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/welsakka/ArabicEnglishLectureModel/blob/main/Whisper_Arabic_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJyQv0P0dg_S"
      },
      "source": [
        "## WHISPER ARABIC TRAINING MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit for this script goes to: https://www.indiumsoftware.com/blog/whisper-ai-model-training-on-custom-data/\n",
        "\n",
        "With a few changes made for Google Colab compatibility"
      ],
      "metadata": {
        "id": "G9ee8cTvXbSy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvHLkv3zdnk_"
      },
      "source": [
        "### IMPORT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUTu7nwfd1sC"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install jiwer\n",
        "!pip install accelerate -U\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import Audio\n",
        "\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUyaRwD6ekpB"
      },
      "source": [
        "### IMPORT TRAINING DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh6oBPsuerEg"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "gdown.download('link/to/training_data.zip', 'init.zip', quiet=False)\n",
        "!unzip init.zip -d ./init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmB8GquPWI1L"
      },
      "outputs": [],
      "source": [
        "import IPython.display\n",
        "IPython.display.Audio(\"/exampleAudio.mp3\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrp0bzBmXUdR"
      },
      "source": [
        "### PREPROCESS AND EXPLORE THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKwUrj0uYsz6"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/path/to/training.csv\")\n",
        "\n",
        "test_df = pd.read_csv(\"/path/to/testing.csv\")\n",
        "\n",
        "## we will rename the columns as \"audio\", \"sentence\".\n",
        "\n",
        "train_df.columns = [\"audio\", \"sentence\"]\n",
        "\n",
        "test_df.columns = [\"audio\", \"sentence\"]\n",
        "\n",
        "#Now we will create the dataset using the class methods Dataset.from_pandas() and cast the audio to an Audio datatype. For example:\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "#We will create arrays of each audio file and append those values as a column in the above datasets.\n",
        "#To do this we will use the cast_column function from Dataset.\n",
        "#We will also use sampling_rate as an argument so if there is any file we missed in preprocessing step.\n",
        "\n",
        "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "test_dataset = test_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLcsswIi98zV"
      },
      "outputs": [],
      "source": [
        "###  Padding and spectrogram conversion are both handled by the Transformers Whisper feature extractor in a single line of code!\n",
        "###  To prepare for our audio data, let’s now load the feature extractor from the pre-trained checkpoint:\n",
        "\n",
        "## import feature extractor\n",
        "\n",
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "## Load WhisperTokenizer\n",
        "\n",
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-base\", language=\"English\", task=\"transcribe\")\n",
        "\n",
        "#Combine To Create A WhisperProcessor\n",
        "#We can combine the tokenizer and feature extractor into a single WhisperProcessor class to make using them easier.\n",
        "#This processor object can be applied to audio inputs and model predictions as necessary and derives from the WhisperFeatureExtractor and WhisperProcessor.\n",
        "\n",
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\", language=\"English\", task=\"transcribe\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsQIX8I1-ypM"
      },
      "source": [
        "### Now we can write a function to prepare our data ready for the model:\n",
        "\n",
        "### Using the batch[„audio] function, we load and resample the audio data. Datasets, as previously mentioned, carry out any necessary resampling operations in real time. From our 1-dimensional audio array, we compute the log-Mel spectrogram input features using the feature extractor. Using the tokenizer, we encode the transcriptions to create label ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKjGXCUdAUp4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_dataset(examples):\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "\n",
        "    audio = examples[\"audio\"]\n",
        "\n",
        "    examples[\"input_features\"] = feature_extractor(\n",
        "\n",
        "        audio[\"array\"], sampling_rate=16000).input_features[0]\n",
        "\n",
        "    del examples[\"audio\"]\n",
        "\n",
        "    sentences = examples[\"sentence\"]\n",
        "    sentences = sentences[:448]\n",
        "\n",
        "    # encode target text to label ids\n",
        "\n",
        "    examples[\"labels\"] = tokenizer(sentences).input_ids\n",
        "\n",
        "    del examples[\"sentence\"]\n",
        "\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFw7Ayu2AnJX"
      },
      "source": [
        "\n",
        "### As we don’t need to carry this data, we are deleting the examples[\"audio\"] and examples[\"labels\"].\n",
        "### Additionally, by erasing this data, RAM space is made available.\n",
        "### Using the dataset’s.map method, we can apply the data preparation function to each of our training examples;\n",
        "### this procedure will take 30 to 40 minutes. Additionally, check that your disc has between 30 and 40 GB of free space, as the map function will attempt to write data to the disc for a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PcVgxa4lBRF3"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(prepare_dataset, num_proc=1)\n",
        "\n",
        "test_dataset = test_dataset.map(prepare_dataset, num_proc=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a data collator"
      ],
      "metadata": {
        "id": "DQboYMz0Y0cy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoQD3zbbWjGa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # pad the labels to max length\n",
        "\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "\n",
        "        # cut bos token here as it’s append later anyways\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jeFJIUUWnLF"
      },
      "outputs": [],
      "source": [
        "#Let’s initialise the data collator we’ve just defined:\n",
        "\n",
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vqgelCwWpGz"
      },
      "source": [
        "##Evaluation Metrics\n",
        "### The evaluation metric that will be applied to our evaluation set is then defined.\n",
        "### The \"de-facto\" metric for rating ASR systems, the Word Error Rate (WER) metric, will be used.\n",
        "### Consult the WER docs for more details. The WER metric will be loaded from Evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "tzCOYBOgZDRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Following that, all we need to do is define a function that takes the model predictions as input and outputs the WER metric.\n",
        "### In the label ids, this function, compute metrics, first replaces -100 for the pad_token_id (undoing the step we applied in the data collator to ignore padded tokens correctly in the loss).\n",
        " ### The predicted and label_ids are then converted to strings. The WER between the predictions and reference labels is calculated at the end."
      ],
      "metadata": {
        "id": "wU8UcayoZEBo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z7J4ExuW56U"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0gdZH-xXula"
      },
      "source": [
        "#Load a Pre-Trained Checkpoint\n",
        "\n",
        " ### Now let’s load the pre-trained Whisper base checkpoint. Again, this is trivial through use of  Transformers!\n",
        "\n",
        "### You can either use a pre-trained model or choose a checkpoint in your local drive. Use one or the other code block for loading the model, DO NOT USE BOTH.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TGVzoYhdaCKK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On7ImPRJX6Uv"
      },
      "outputs": [],
      "source": [
        "#from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "#PRE TRAINED\n",
        "\n",
        "#model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
        "\n",
        "'''\n",
        "Before autoregressive generation begins, the Whisper model generates (forced_decoder_ids).\n",
        "which are token ids that are required as model outputs.\n",
        "These token ids regulate the transcription language and task for zero-shot ASR.\n",
        "As we’ll train the model to predict the correct language and task,\n",
        "we’ll set these ids to None for fine-tuning (transcription).\n",
        "Additionally, some tokens (suppress_tokens) are entirely suppressed during generation.\n",
        "'''\n",
        "\n",
        "#model.config.forced_decoder_ids = None\n",
        "#model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rb_eS4ngw0E"
      },
      "outputs": [],
      "source": [
        "# Mount google drive to output checkpoint file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#checkpoint\n",
        "\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"/path/to/checkpoint-xxxx\")\n",
        "\n",
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxhdi_kyYaSj"
      },
      "source": [
        "\n",
        "## Define the Training Arguments\n",
        "\n",
        "###We define all the training-related parameters in the last step. The following is an explanation of a subset of parameters:\n",
        "\n",
        " ###   output_dir: is a local directory in which you can save the model weights. This will be the repository name on the Hugging Face Hub.\n",
        "  ### generation_max_length: maximum number of tokens to autoregressive generate throughout assessment.\n",
        "  ###  save_steps: during training, intermediate checkpoints can be saved and also be uploaded asynchronously to the Hub every save_steps training steps.\n",
        "  ###  eval_steps: during training, evaluation of intermediate checkpoints will be accomplished every eval_steps training steps.\n",
        "  ###  report_to: where to save training logs. Supported platforms are \"azure_ml\", \"comet_ml\", \"mlflow\", \"neptune\", \"tensorboard\" and \"wandb\". Pick your favorite or leave it as \"tensorboard\" to log to the Hub.\n",
        "  ###  Push_to_hub: Since we don’t want to force our models into hugging faces, we set this value to False.\n",
        "\n",
        "###Contact the Seq2SeqTrainingArguments docs for more information on the other training arguments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTPpUG9Ecf5a"
      },
      "outputs": [],
      "source": [
        "# Mount google drive to output checkpoint file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the Training Arguments\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "\n",
        "    output_dir=\"/path/to/training/output/\",  # change to a repo name of your choice\n",
        "\n",
        "    per_device_train_batch_size=16,\n",
        "\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "\n",
        "    learning_rate=1e-5,\n",
        "\n",
        "    warmup_steps=500,\n",
        "\n",
        "    max_steps=4500,\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "\n",
        "    fp16=True,\n",
        "\n",
        "    evaluation_strategy=\"steps\",\n",
        "\n",
        "    per_device_eval_batch_size=1,\n",
        "\n",
        "    predict_with_generate=True,\n",
        "\n",
        "    generation_max_length=225,\n",
        "\n",
        "    save_steps=500,\n",
        "\n",
        "    eval_steps=500,\n",
        "\n",
        "    # logging_steps=25,\n",
        "\n",
        "    report_to=[\"tensorboard\"],\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    metric_for_best_model=\"wer\",\n",
        "\n",
        "    greater_is_better=False,\n",
        "\n",
        "    push_to_hub=False,\n",
        "\n",
        ")\n",
        "\n",
        "#Along with our model, dataset, data collator, and compute_metrics function, we can send the training arguments to the Trainer:\n",
        "\n",
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "\n",
        "    args=training_args,\n",
        "\n",
        "    model=model,\n",
        "\n",
        "    train_dataset=train_dataset,\n",
        "\n",
        "    eval_dataset=test_dataset,\n",
        "\n",
        "    data_collator=data_collator,\n",
        "\n",
        "    compute_metrics=compute_metrics,\n",
        "\n",
        "    tokenizer=processor.feature_extractor,\n",
        "\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtHYkqeQYj8f"
      },
      "outputs": [],
      "source": [
        "#To launch training, simply execute:\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnwfCMt5kGBC"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMaR83xvl79/Qaic7kxa3K",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}